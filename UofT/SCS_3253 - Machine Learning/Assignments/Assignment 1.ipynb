{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your name:\n",
    "\n",
    "<pre> SV</pre>\n",
    "\n",
    "### Collaborators:\n",
    "\n",
    "<pre> Enter the name of the people you worked with if any</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown referece can be found here:\n",
    "    http://nestacms.com/docs/creating-content/markdown-cheat-sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. Why would it be a problem if our training set and test set are the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In general, the objective of machine learning is to build a <b>generalized model</b> capable of making decisions or predictions regarding 'unseen' data based on 'learnings' from a training dataset. </p>\n",
    "\n",
    "<p>Most real world systems change on a constant basis and consequently the data that they generate also changes. While \n",
    "building a machine learning model that consumes such data, it is important to acknowledge and account for the 'changing' nature of the data by using a completely unseen dataset to evaluate the performance of the model. If we were to both train and test \n",
    "the model on the exact same data points, the model would undoubtedly seem to perform very well since it would have identified\n",
    "the nuances within the training data and when these same data points are presented again, produce an output that very closely\n",
    "matches the expected value. This is known as <b><i>'over-fitting'</i></b>.</p>\n",
    "\n",
    "<p>To get a more realistic measure of performance, therefore, it is important to keep a portion of the training dataset <i>hidden</i> from the model and train it on the remaining data. This technique is called the <b><i>'holdout method'</i></b>. One could go a step further and improve the results of this technique by performing multiple iterations of this method, each time using a different part of the data to train and the remaining to test the model. This is known as <b>k-fold cross-validation</b>.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2 [OPTIONAL]. Explain step by step the process to select k in the k-nearest neighbor algorithm (pseudocode) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> ENTER SOLUTION HERE </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. For the k-nearest regression. What happends when k = n. Where n is equal to the training size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The choice of k value plays a critical role in the output of the K-nearest neighbors (KNN) algorithm. In essence, the KNN algorithm divides the feature space into 'classes' of related points. Choosing a low K value (for ex. k=1) would result in very sharp class boundaries and in general lead to overfitting. There would also be a greater amount of variance among the classes but lower bias in terms of mis- classifying or regressing a test point.</p> \n",
    "<p>On the other hand, choosing a very high k value (for ex. k=n, where n is equal to the size of the training dataset) would result in highly smoothed class boundaries. There will be a lower amount of variance overall but a greater level of bias leading to greater levels of mis-classification or inaccurate regression of a test point. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. Define a function that takes a 1-d numpy array, a parameter k, and a number p.\n",
    "The function returns an estimate equal to the mean of the closest k points to the number p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_neighbor(input_data, k, p):\n",
    "    \"\"\"Returns the k-neighbor estimate for p using data input_data.\n",
    "    \n",
    "    Keyword arguments:\n",
    "    input_data -- numpy array of all the data\n",
    "    k -- Number of k\n",
    "    p -- input values\n",
    "    \n",
    "    If you make assumptions please explain them in the comments. i.e. tie breaking strategy.\n",
    "    \"\"\"\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    # STEP 1: compute distances to all points in the feature space as absolute value of difference\n",
    "    # between p and each point\n",
    "    distances = [(i, abs(p - i)) for i in data]\n",
    " \n",
    "    # STEP 2: sort in ascending order of distances\n",
    "    # Tie Breaker logic: we sort the list of tuples first by absolute distance and \n",
    "    # then by the value of each point. This way, if two points lie at the same distance from\n",
    "    # the test point, the one with lower value will be given preference i.e. we are making a 'conservative'\n",
    "    # estimation.\n",
    "    distances_sorted = sorted(distances, key=lambda tup: (tup[1], tup[0]))\n",
    "    \n",
    "    # STEP 3: find k closest points\n",
    "    k_closest_points = ([tup[0] for tup in distances_sorted[:k]])\n",
    "    print('Closest', k, 'points to', p, ':', k_closest_points)\n",
    "    \n",
    "    # STEP 4: find mean of k closes points\n",
    "    k_neighbor_estmt = np.mean(k_closest_points)\n",
    "    \n",
    "    #return answer\n",
    "    return k_neighbor_estmt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest 3 points to 5 : [5, 4, 3]\n",
      "4.0\n",
      "Closest 2 points to 15 : [15, 13]\n",
      "14.0\n",
      "Closest 3 points to 25 : [25, 24, 29]\n",
      "26.0\n",
      "Closest 1 points to 55 : [40]\n",
      "40.0\n",
      "Closest 3 points to 55 : [40, 29, 25]\n",
      "31.3333333333\n",
      "Closest 10 points to 55 : [40, 29, 25, 24, 19, 15, 13, 12, 11, 8]\n",
      "19.6\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "data = np.array([1,3,4,5,7,8,11,12,13,15,19,24,25,29,40])\n",
    "print(k_neighbor(input_data=data, k=3, p=5))\n",
    "print(k_neighbor(input_data=data, k=2, p=15))\n",
    "print(k_neighbor(input_data=data, k=3, p=25))\n",
    "print(k_neighbor(input_data=data, k=1, p=55))\n",
    "print(k_neighbor(input_data=data, k=3, p=55))\n",
    "print(k_neighbor(input_data=data, k=10, p=55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter your observations and comments here\n",
    "# (1) Low values of K, for example k = 1 as in example 4 seems to result in more accurate regressing. \n",
    "# (2) High values of K, for example k = 10 as in the last example seems to lead to very inaccurate \n",
    "# regressing: considerable difference between the test point of 55 and the output of 19.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Similar to Q4 but for the n dimentional case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_norm(a,b):\n",
    "    \"\"\"Returns the l1 norm (a,b)\"\"\"\n",
    "\n",
    "    return np.sum([abs(x[0] - x[1]) for x in zip(a, b)])\n",
    "    \n",
    "def l2_norm(a,b):\n",
    "    \"\"\"Returns the l2 norm (a,b)\"\"\" \n",
    "    \n",
    "    return np.sqrt(np.sum([np.square(x[0] - x[1]) for x in zip(a, b)]))\n",
    "    \n",
    "def k_neighbor_nd(input_data, k, p, metric='l1', mode='mean'):\n",
    "    \"\"\"Returns the k-neighbor estimate for p using data input_data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    input_data -- numpy array of all the data\n",
    "    k -- Number of k\n",
    "    p -- input values\n",
    "    metric -- l1 or l2. l1 norm or l2 norm https://en.wikipedia.org/wiki/Norm_(mathematics)\n",
    "    mode -- estimator possible values = 'mean', 'median', 'max'\n",
    "    \n",
    "    Implement the l1 and l2 norms\n",
    "    for mean, median and max, use np.mean, np.median and np.max\n",
    "    \"\"\"\n",
    "    \n",
    "    #YOUR CODE HERE\n",
    "    # STEP 0: define function maps\n",
    "    metric_func_map = {'l1': l1_norm,\n",
    "                     'l2': l2_norm}\n",
    "    mode_func_map = {'mean': np.mean,\n",
    "                     'median': np.median,\n",
    "                     'max': np.max}\n",
    "    \n",
    "    # STEP 1: compute distances from input p to all points in the feature space\n",
    "    # based on input.\n",
    "    distances = [(y[1], norm_func_map[metric](y[0], y[1])) for y in [(p, x) for x in data_4d]]\n",
    "        \n",
    "    # STEP 2: sort in ascending order of distances\n",
    "    distances_sorted = sorted(distances, key=lambda tup: tup[1])\n",
    "    print(distances_sorted)\n",
    "    \n",
    "    # STEP 3: find k closest points\n",
    "    # Tie-breaker logic: here in case of a tie between two n-dimensional points, we pick one at random i.e.\n",
    "    # in the order that the sort function ordered the points after sorting by distance\n",
    "    k_closest_points = ([tup[0] for tup in distances_sorted[:k]])\n",
    "    print('Closest', k, 'points to', p, ':', k_closest_points)\n",
    "    \n",
    "    # STEP 4: find mean/median/max of k closest points based on inpit\n",
    "    print('Using mode:', mode )\n",
    "    k_neighbor_estmt = [x for x in mode_func_map[mode](k_closest_points, axis=0)]\n",
    "    \n",
    "    #return answer\n",
    "    return k_neighbor_estmt\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_4d = np.array([[4, 1, 2, 1], [1, 4, 2, 0], [3, 3, 1, 1], \n",
    "        [4, 0, 0, 0], [1, 2, 0, 0], [3, 4, 2, 3], \n",
    "        [2, 4, 4, 2], [2, 1, 4, 1], [3, 3, 2, 4], \n",
    "        [4, 3, 0, 4], [2, 2, 4, 0],[4, 3, 0, 2], \n",
    "        [4, 3, 0, 2], [0, 3, 4, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest 3 points to [2, 1, 4, 3] : [array([2, 1, 4, 1]), array([2, 4, 4, 2]), array([2, 2, 4, 0])]\n",
      "Using mode: mean\n",
      "[2.0, 2.3333333333333335, 4.0, 1.0]\n",
      "Closest 2 points to [4, 4, 0, 0] : [array([4, 3, 0, 2]), array([4, 3, 0, 2])]\n",
      "Using mode: mean\n",
      "[4.0, 3.0, 0.0, 2.0]\n",
      "Closest 3 points to [2, 2, 2, 4] : [array([3, 3, 2, 4]), array([3, 4, 2, 3]), array([4, 3, 0, 4])]\n",
      "Using mode: max\n",
      "[4, 4, 2, 4]\n",
      "Closest 1 points to [2, 3, 3, 4] : [array([3, 3, 2, 4])]\n",
      "Using mode: mean\n",
      "[3.0, 3.0, 2.0, 4.0]\n",
      "Closest 3 points to [2, 3, 3, 4] : [array([3, 3, 2, 4]), array([3, 4, 2, 3]), array([2, 4, 4, 2])]\n",
      "Using mode: median\n",
      "[3.0, 4.0, 2.0, 3.0]\n",
      "Closest 3 points to [2, 1, 4, 3] : [array([2, 1, 4, 1]), array([0, 3, 4, 2]), array([2, 4, 4, 2])]\n",
      "Using mode: mean\n",
      "[1.3333333333333333, 2.6666666666666665, 4.0, 1.6666666666666667]\n",
      "Closest 2 points to [4, 4, 0, 0] : [array([3, 3, 1, 1]), array([4, 3, 0, 2])]\n",
      "Using mode: mean\n",
      "[3.5, 3.0, 0.5, 1.5]\n",
      "Closest 3 points to [2, 2, 2, 4] : [array([3, 3, 2, 4]), array([3, 4, 2, 3]), array([4, 3, 0, 4])]\n",
      "Using mode: max\n",
      "[4, 4, 2, 4]\n",
      "Closest 1 points to [2, 3, 3, 4] : [array([3, 3, 2, 4])]\n",
      "Using mode: mean\n",
      "[3.0, 3.0, 2.0, 4.0]\n",
      "Closest 3 points to [2, 3, 3, 4] : [array([3, 3, 2, 4]), array([3, 4, 2, 3]), array([2, 4, 4, 2])]\n",
      "Using mode: median\n",
      "[3.0, 4.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate\n",
    "data = np.array([1,3,4,5,7,8,11,12,13,15,19,24,25,29,40])\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 1, 4, 3], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=2, p=[4, 4, 0, 0], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 2, 2, 4], metric='l1', mode='max'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=1, p=[2, 3, 3, 4], metric='l1', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 3, 3, 4], metric='l1', mode='median'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 1, 4, 3], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=2, p=[4, 4, 0, 0], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 2, 2, 4], metric='l2', mode='max'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=1, p=[2, 3, 3, 4], metric='l2', mode='mean'))\n",
    "print(k_neighbor_nd(input_data=data_4d, k=3, p=[2, 3, 3, 4], metric='l2', mode='median'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Enter your observations and comments here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6[Optional]. Read the documentation on KNeighborsRegressor\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "    \n",
    "Explore the source code:\n",
    "    https://github.com/scikit-learn/scikit-learn/blob/ef5cb84a/sklearn/neighbors/regression.py\n",
    "        \n",
    "How different it is from your implementation? How well can you follow the code? Did you learn something new?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> ENTER SOLUTION HERE </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Solution\n",
    "\n",
    "#Question 1\n",
    "Remember the puporse of machine learning is to be able to generalize with data the algorithm has never seen. \n",
    "If you train and test with the same data, you are evaluating the performance on the same peace of data. \n",
    "Therefore, you would not be able to know whether the algorithm actually would perform on unseen data. 20 / 20\n",
    "\n",
    "#Question 3\n",
    "If we set k = n, the prediction will be the same for any new data point -> Average of all poins in the training set. 15 / 20\n",
    "\n",
    "#Question 4\n",
    "Code\n",
    "arr = (((p - input_data)2))0.5\n",
    "ind = np.argpartition(arr, k)[:k]\n",
    "return(np.mean(input_data[ind]))\n",
    "Results\n",
    "5.333333333333333\n",
    "14.0\n",
    "26.0\n",
    "40.0\n",
    "31.333333333333332\n",
    "> 19.6\n",
    "\n",
    "30 / 30\n",
    "\n",
    "#Question 5\n",
    "Code\n",
    "def l1_norm(a,b): \"\"\"Returns the l1 norm (a,b)\"\"\" return sum(abs(a - b))\n",
    "\n",
    "def l2_norm(a,b): \"\"\"Returns the l2 norm (a,b)\"\"\" return (sum((b - a)2))0.5\n",
    "\n",
    "def k_neighbor_nd(input_data, k, p, metric='l1', mode='mean'): \"\"\"Returns the k-neighbor estimate for p using data input_data.\n",
    "\n",
    "Keyword arguments:\n",
    "input_data -- numpy array of all the data\n",
    "k -- Number of k\n",
    "p -- input values\n",
    "metric -- l1 or l2. l1 norm or l2 norm https://en.wikipedia.org/wiki/Norm_(mathematics)\n",
    "mode -- estimator possible values = 'mean', 'median', 'max'\n",
    "\n",
    "Implement the l1 and l2 norms\n",
    "for mean, median and max, use np.mean, np.median and np.max\n",
    "\"\"\"\n",
    "\n",
    "#YOUR CODE HERE\n",
    "if metric == 'l1':\n",
    "    arr = np.zeros(0)\n",
    "    for i in range(0, input_data.shape[0]):\n",
    "        arr = np.append(arr, l1_norm(p, input_data[i]))\n",
    "\n",
    "if metric == 'l2':\n",
    "    arr = np.zeros(0)\n",
    "    for i in range(0, input_data.shape[0]):\n",
    "        arr = np.append(arr, l2_norm(p, input_data[i]))\n",
    "\n",
    "\n",
    "ind = np.argpartition(arr, k)[:k]\n",
    "\n",
    "if mode == 'mean':\n",
    "    return(np.mean(input_data[ind], axis = 0))\n",
    "\n",
    "if mode == 'median':\n",
    "    return(np.median(input_data[ind], axis = 0))\n",
    "\n",
    "if mode == 'max':\n",
    "    return(np.max(input_data[ind], axis = 0))\n",
    "Results\n",
    "[ 2. 2.33333333 4. 1. ]\n",
    "[ 4. 3. 0. 2.]\n",
    "[4 4 2 4]\n",
    "[ 3. 3. 2. 4.]\n",
    "[ 3. 4. 2. 3.]\n",
    "[ 1.33333333 2. 4. 1. ]\n",
    "[ 3.5 3. 0.5 1.5]\n",
    "[4 4 2 4]\n",
    "[ 3. 3. 2. 4.]\n",
    "[ 3. 4. 2. 3.]\n",
    "\n",
    "30 / 30\n",
    "\n",
    "Total : 95 / 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
